{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64abe31d-e8aa-4c34-a442-b327658932f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TARGETED HIGH-RISK CLASS PERFORMANCE IMPROVEMENT\n",
      "============================================================\n",
      "Focus: Boosting Precision, Recall, and F1 for High-Risk Nutritional Patients\n",
      "\n",
      "1. ANALYZING CLASS IMBALANCE PROBLEM\n",
      "==================================================\n",
      "Loaded data: (179, 86)\n",
      "Class Distribution Analysis:\n",
      "  Low Risk (0):  116 samples (64.8%)\n",
      "  High Risk (1): 63 samples (35.2%)\n",
      "  Imbalance Ratio: 1.8:1 (Low:High)\n",
      "  MILD IMBALANCE - Basic class weighting sufficient\n",
      "\n",
      "2. STRATEGIC OVERSAMPLING WITH SMART VARIATION\n",
      "==================================================\n",
      "Original: 116 low-risk, 63 high-risk\n",
      "Creating 63 synthetic high-risk samples...\n",
      "Balanced: 116 low-risk, 126 high-risk\n",
      "Original: 116 low-risk, 63 high-risk\n",
      "No oversampling needed for moderate strategy\n",
      "Original: 116 low-risk, 63 high-risk\n",
      "Creating 14 synthetic high-risk samples...\n",
      "Balanced: 116 low-risk, 77 high-risk\n",
      "\n",
      "3. HIGH-RECALL OPTIMIZED MODELS\n",
      "==================================================\n",
      "TESTING ON ORIGINAL IMBALANCED DATA:\n",
      "Testing High-Weight Logistic Regression...\n",
      "\n",
      "Weighted Logistic Regression Results:\n",
      "  AUC: 0.648\n",
      "  High-Risk Precision: 0.394\n",
      "  High-Risk Recall:    0.684\n",
      "  High-Risk F1:        0.500\n",
      "Testing Recall-Optimized Random Forest...\n",
      "\n",
      "Recall-Optimized Random Forest Results:\n",
      "  AUC: 0.562\n",
      "  High-Risk Precision: 1.000\n",
      "  High-Risk Recall:    0.158\n",
      "  High-Risk F1:        0.273\n",
      "Testing Gradient Boosting...\n",
      "\n",
      "Custom Gradient Boosting Results:\n",
      "  AUC: 0.696\n",
      "  High-Risk Precision: 0.500\n",
      "  High-Risk Recall:    0.316\n",
      "  High-Risk F1:        0.387\n",
      "Testing Weighted SVM...\n",
      "\n",
      "Weighted SVM Results:\n",
      "  AUC: 0.326\n",
      "  High-Risk Precision: 0.500\n",
      "  High-Risk Recall:    0.474\n",
      "  High-Risk F1:        0.486\n",
      "\n",
      "TESTING ON CONSERVATIVE BALANCED DATA:\n",
      "Testing High-Weight Logistic Regression...\n",
      "\n",
      "Weighted Logistic Regression Results:\n",
      "  AUC: 0.724\n",
      "  High-Risk Precision: 0.579\n",
      "  High-Risk Recall:    0.868\n",
      "  High-Risk F1:        0.695\n",
      "Testing Recall-Optimized Random Forest...\n",
      "\n",
      "Recall-Optimized Random Forest Results:\n",
      "  AUC: 0.753\n",
      "  High-Risk Precision: 0.703\n",
      "  High-Risk Recall:    0.684\n",
      "  High-Risk F1:        0.693\n",
      "Testing Gradient Boosting...\n",
      "\n",
      "Custom Gradient Boosting Results:\n",
      "  AUC: 0.774\n",
      "  High-Risk Precision: 0.839\n",
      "  High-Risk Recall:    0.684\n",
      "  High-Risk F1:        0.754\n",
      "Testing Weighted SVM...\n",
      "\n",
      "Weighted SVM Results:\n",
      "  AUC: 0.829\n",
      "  High-Risk Precision: 0.732\n",
      "  High-Risk Recall:    0.789\n",
      "  High-Risk F1:        0.759\n",
      "\n",
      "4. THRESHOLD OPTIMIZATION FOR HIGH RECALL\n",
      "==================================================\n",
      "Optimizing threshold for: Weighted_LR\n",
      "Original performance: Recall=0.684, Precision=0.394\n",
      "Target Recall 0.70: Threshold=0.475, Achieved Recall=0.737, Precision=0.400\n",
      "Target Recall 0.75: Threshold=0.363, Achieved Recall=0.842, Precision=0.390\n",
      "Target Recall 0.80: Threshold=0.363, Achieved Recall=0.842, Precision=0.390\n",
      "Target Recall 0.85: Threshold=0.099, Achieved Recall=1.000, Precision=0.380\n",
      "\n",
      "5. SPECIALIZED ENSEMBLE FOR HIGH-RISK DETECTION\n",
      "==================================================\n",
      "Ensemble components:\n",
      "  Weighted_LR: Weight=0.270 (Recall=0.684)\n",
      "\n",
      "Ensemble Results:\n",
      "  Optimal Threshold: 0.800\n",
      "  AUC: 0.648\n",
      "  High-Risk Precision: 0.520\n",
      "  High-Risk Recall:    0.684\n",
      "  High-Risk F1:        0.591\n",
      "\n",
      "6. PERFORMANCE COMPARISON AND RECOMMENDATIONS\n",
      "==================================================\n",
      "HIGH-RISK CLASS PERFORMANCE RANKING:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank Model                     Precision  Recall   F1       AUC     \n",
      "--------------------------------------------------------------------------------\n",
      "1    conservative_Weighted_SV  0.732      0.789    0.759    0.829   \n",
      "2    conservative_Custom_GB    0.839      0.684    0.754    0.774   \n",
      "3    conservative_Weighted_LR  0.579      0.868    0.695    0.724   \n",
      "4    conservative_Recall_RF    0.703      0.684    0.693    0.753   \n",
      "5    Ensemble                  0.520      0.684    0.591    0.648   \n",
      "6    Optimized_Weighted_LR     0.394      0.684    0.500    0.648   \n",
      "7    Optimized_Weighted_SVM    0.500      0.474    0.486    0.326   \n",
      "8    Optimized_Custom_GB       0.500      0.316    0.387    0.696   \n",
      "9    Optimized_Recall_RF       1.000      0.158    0.273    0.562   \n",
      "\n",
      "üèÜ BEST HIGH-RISK PERFORMANCE: conservative_Weighted_SVM\n",
      "   High-Risk Precision: 0.732\n",
      "   High-Risk Recall:    0.789\n",
      "   High-Risk F1:        0.759\n",
      "   ‚úÖ EXCELLENT: Catches 75%+ of high-risk patients\n",
      "   ‚úÖ EXCELLENT: Low false alarm rate\n",
      "\n",
      "üìÅ SAVING BEST MODEL FOR FUTURE USE\n",
      "==================================================\n",
      "   ‚úÖ Scaling required - StandardScaler saved (trained on conservative data)\n",
      "‚úÖ Model saved successfully: patient_predictor_model.pkl\n",
      "\n",
      "üìã MODEL PACKAGE CONTENTS:\n",
      "   ‚Ä¢ Model Type: conservative_Weighted_SVM\n",
      "   ‚Ä¢ Requires Scaling: True\n",
      "   ‚Ä¢ Features Used: 82 features\n",
      "   ‚Ä¢ Performance: F1=0.759\n",
      "   ‚Ä¢ Random State: 42\n",
      "\n",
      "============================================================\n",
      "HIGH-RISK CLASS IMPROVEMENT ANALYSIS COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import random\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# CRITICAL: SET ALL RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# =====================================================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Set all random seeds to ensure reproducible results\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\nTARGETED HIGH-RISK CLASS PERFORMANCE IMPROVEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Focus: Boosting Precision, Recall, and F1 for High-Risk Nutritional Patients\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 1: ANALYZE CURRENT IMBALANCE PROBLEM\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n1. ANALYZING CLASS IMBALANCE PROBLEM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df_clustered = pd.read_csv('Binary_RF_SVD_clustering_results.csv')\n",
    "    print(f\"Loaded data: {df_clustered.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Please ensure SVD clustering results are available\")\n",
    "    exit()\n",
    "\n",
    "# Prepare features and target\n",
    "svd_cols = [col for col in df_clustered.columns if col.startswith('SVD_Component')]\n",
    "df_modeling = df_clustered.copy()\n",
    "\n",
    "feature_cols = ['Gender', 'Age'] + svd_cols\n",
    "X = df_modeling[feature_cols].fillna(0)\n",
    "y = (df_modeling['MUSTScore'] >= 2).astype(int)\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = Counter(y)\n",
    "total_samples = len(y)\n",
    "high_risk_count = class_counts[1]\n",
    "low_risk_count = class_counts[0]\n",
    "imbalance_ratio = low_risk_count / high_risk_count\n",
    "\n",
    "print(f\"Class Distribution Analysis:\")\n",
    "print(f\"  Low Risk (0):  {low_risk_count} samples ({low_risk_count/total_samples:.1%})\")\n",
    "print(f\"  High Risk (1): {high_risk_count} samples ({high_risk_count/total_samples:.1%})\")\n",
    "print(f\"  Imbalance Ratio: {imbalance_ratio:.1f}:1 (Low:High)\")\n",
    "\n",
    "if imbalance_ratio > 5:\n",
    "    print(f\"  SEVERE IMBALANCE DETECTED - Need aggressive techniques\")\n",
    "elif imbalance_ratio > 3:\n",
    "    print(f\"  MODERATE IMBALANCE - Standard techniques should work\")\n",
    "else:\n",
    "    print(f\"  MILD IMBALANCE - Basic class weighting sufficient\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 2: STRATEGIC OVERSAMPLING WITH VARIATION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n2. STRATEGIC OVERSAMPLING WITH SMART VARIATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def create_smart_synthetic_samples(X_minority, y_minority, n_synthetic=50, noise_level=0.1, random_state=None):\n",
    "    \"\"\"Create synthetic samples with small variations from existing high-risk patients\"\"\"\n",
    "    \n",
    "    # Set local random state for this function\n",
    "    if random_state is not None:\n",
    "        local_rng = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        local_rng = np.random.RandomState(RANDOM_STATE + 100)  # Offset to avoid conflicts\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    X_min_array = X_minority.values\n",
    "    \n",
    "    for i in range(n_synthetic):\n",
    "        # Use local_rng for reproducibility\n",
    "        base_idx = local_rng.randint(0, len(X_min_array))\n",
    "        base_sample = X_min_array[base_idx].copy()\n",
    "        \n",
    "        # Add small random variations\n",
    "        for j in range(len(base_sample)):\n",
    "            if local_rng.random() < 0.3:  # 30% chance to modify each feature\n",
    "                if j < 2:  # Demographic features (Gender, Age)\n",
    "                    if j == 0:  # Gender - keep unchanged\n",
    "                        continue\n",
    "                    else:  # Age - small variation\n",
    "                        base_sample[j] += local_rng.normal(0, 2)  # +/- 2 years\n",
    "                else:  # SVD components - small noise\n",
    "                    base_sample[j] += local_rng.normal(0, abs(base_sample[j]) * noise_level)\n",
    "        \n",
    "        synthetic_samples.append(base_sample)\n",
    "        synthetic_labels.append(1)  # All are high-risk\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    synthetic_df = pd.DataFrame(synthetic_samples, columns=X_minority.columns)\n",
    "    \n",
    "    return synthetic_df, np.array(synthetic_labels)\n",
    "\n",
    "def create_balanced_dataset(X, y, strategy='moderate'):\n",
    "    \"\"\"Create balanced dataset with different intensity levels\"\"\"\n",
    "    \n",
    "    # Separate classes\n",
    "    X_low_risk = X[y == 0]\n",
    "    X_high_risk = X[y == 1]\n",
    "    y_low_risk = y[y == 0]\n",
    "    y_high_risk = y[y == 1]\n",
    "    \n",
    "    print(f\"Original: {len(X_low_risk)} low-risk, {len(X_high_risk)} high-risk\")\n",
    "    \n",
    "    if strategy == 'conservative':\n",
    "        # Increase high-risk by 2x\n",
    "        target_high_risk = len(X_high_risk) * 2\n",
    "    elif strategy == 'moderate':\n",
    "        # Increase high-risk to 1:2 ratio (instead of 1:7)\n",
    "        target_high_risk = len(X_low_risk) // 2\n",
    "    elif strategy == 'aggressive':\n",
    "        # Near balance: 1:1.5 ratio\n",
    "        target_high_risk = int(len(X_low_risk) * 0.67)\n",
    "    else:\n",
    "        target_high_risk = len(X_high_risk)\n",
    "    \n",
    "    # Calculate how many synthetic samples needed\n",
    "    n_synthetic = max(0, target_high_risk - len(X_high_risk))\n",
    "    \n",
    "    if n_synthetic > 0:\n",
    "        print(f\"Creating {n_synthetic} synthetic high-risk samples...\")\n",
    "        \n",
    "        # Create synthetic samples with reproducible random state\n",
    "        strategy_seed = RANDOM_STATE + {'conservative': 10, 'moderate': 20, 'aggressive': 30}.get(strategy, 0)\n",
    "        X_synthetic, y_synthetic = create_smart_synthetic_samples(\n",
    "            X_high_risk, y_high_risk, n_synthetic, random_state=strategy_seed\n",
    "        )\n",
    "        \n",
    "        # Combine all data\n",
    "        X_balanced = pd.concat([X_low_risk, X_high_risk, X_synthetic], ignore_index=True)\n",
    "        y_balanced = np.concatenate([y_low_risk, y_high_risk, y_synthetic])\n",
    "        \n",
    "        print(f\"Balanced: {len(X_low_risk)} low-risk, {len(X_high_risk) + n_synthetic} high-risk\")\n",
    "        \n",
    "    else:\n",
    "        X_balanced, y_balanced = X, y\n",
    "        print(f\"No oversampling needed for {strategy} strategy\")\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Create different balanced datasets\n",
    "strategies = ['conservative', 'moderate', 'aggressive']\n",
    "balanced_datasets = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    X_bal, y_bal = create_balanced_dataset(X, y, strategy)\n",
    "    balanced_datasets[strategy] = (X_bal, y_bal)\n",
    "\n",
    "# =====================================================\n",
    "# STEP 3: HIGH-RECALL OPTIMIZED MODELS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n3. HIGH-RECALL OPTIMIZED MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def evaluate_high_risk_focused_model(X_train, X_test, y_train, y_test, model, model_name):\n",
    "    \"\"\"Evaluate model with focus on high-risk class performance\"\"\"\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.decision_function(X_test)\n",
    "    \n",
    "    # Standard metrics\n",
    "    report = classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk'], \n",
    "                                 output_dict=True, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # High-risk focused metrics\n",
    "    high_risk_precision = report['High Risk']['precision']\n",
    "    high_risk_recall = report['High Risk']['recall']\n",
    "    high_risk_f1 = report['High Risk']['f1-score']\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  AUC: {auc:.3f}\")\n",
    "    print(f\"  High-Risk Precision: {high_risk_precision:.3f}\")\n",
    "    print(f\"  High-Risk Recall:    {high_risk_recall:.3f}\")\n",
    "    print(f\"  High-Risk F1:        {high_risk_f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'high_risk_precision': high_risk_precision,\n",
    "        'high_risk_recall': high_risk_recall,\n",
    "        'high_risk_f1': high_risk_f1,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "def test_optimized_algorithms(X, y, test_size=0.3):\n",
    "    \"\"\"Test algorithms specifically optimized for high-risk detection\"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Highly Weighted Logistic Regression\n",
    "    print(\"Testing High-Weight Logistic Regression...\")\n",
    "    class_weights = {0: 1, 1: 8}  # 8x weight for high-risk\n",
    "    lr_weighted = LogisticRegression(\n",
    "        random_state=RANDOM_STATE, \n",
    "        class_weight=class_weights,\n",
    "        C=0.1,  # More regularization\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results['Weighted_LR'] = evaluate_high_risk_focused_model(\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, lr_weighted, \"Weighted Logistic Regression\"\n",
    "    )\n",
    "    \n",
    "    # 2. Recall-Optimized Random Forest\n",
    "    print(\"Testing Recall-Optimized Random Forest...\")\n",
    "    rf_recall = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight={0: 1, 1: 6},  # 6x weight for high-risk\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    results['Recall_RF'] = evaluate_high_risk_focused_model(\n",
    "        X_train, X_test, y_train, y_test, rf_recall, \"Recall-Optimized Random Forest\"\n",
    "    )\n",
    "    \n",
    "    # 3. Gradient Boosting with Custom Loss\n",
    "    print(\"Testing Gradient Boosting...\")\n",
    "    gb_custom = GradientBoostingClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.05,  # Slower learning\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    results['Custom_GB'] = evaluate_high_risk_focused_model(\n",
    "        X_train, X_test, y_train, y_test, gb_custom, \"Custom Gradient Boosting\"\n",
    "    )\n",
    "    \n",
    "    # 4. SVM with Custom Weights\n",
    "    print(\"Testing Weighted SVM...\")\n",
    "    svm_weighted = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        class_weight={0: 1, 1: 7},  # 7x weight for high-risk\n",
    "        probability=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    results['Weighted_SVM'] = evaluate_high_risk_focused_model(\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, svm_weighted, \"Weighted SVM\"\n",
    "    )\n",
    "    \n",
    "    return results, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Test on original data\n",
    "print(\"TESTING ON ORIGINAL IMBALANCED DATA:\")\n",
    "original_results, X_tr, X_te, y_tr, y_te = test_optimized_algorithms(X, y)\n",
    "\n",
    "# Test on balanced datasets\n",
    "balanced_results = {}\n",
    "for strategy, (X_bal, y_bal) in balanced_datasets.items():\n",
    "    if len(X_bal) > len(X) * 1.1:  # Only if we actually added samples\n",
    "        print(f\"\\nTESTING ON {strategy.upper()} BALANCED DATA:\")\n",
    "        strategy_results, _, _, _, _ = test_optimized_algorithms(X_bal, y_bal)\n",
    "        balanced_results[strategy] = strategy_results\n",
    "\n",
    "# =====================================================\n",
    "# STEP 4: THRESHOLD OPTIMIZATION FOR HIGH RECALL\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n4. THRESHOLD OPTIMIZATION FOR HIGH RECALL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def optimize_threshold_for_recall(y_true, y_pred_proba, min_recall=0.80):\n",
    "    \"\"\"Find threshold that maximizes precision while maintaining minimum recall\"\"\"\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    # Find thresholds that meet minimum recall requirement\n",
    "    valid_thresholds = thresholds[recall[:-1] >= min_recall]\n",
    "    valid_precision = precision[:-1][recall[:-1] >= min_recall]\n",
    "    valid_recall = recall[:-1][recall[:-1] >= min_recall]\n",
    "    \n",
    "    if len(valid_thresholds) == 0:\n",
    "        # If no threshold meets min_recall, find best available\n",
    "        best_idx = np.argmax(recall[:-1])\n",
    "        return thresholds[best_idx], precision[best_idx], recall[best_idx]\n",
    "    \n",
    "    # Among valid thresholds, pick the one with highest precision\n",
    "    best_idx = np.argmax(valid_precision)\n",
    "    \n",
    "    return valid_thresholds[best_idx], valid_precision[best_idx], valid_recall[best_idx]\n",
    "\n",
    "# Find best model from original results\n",
    "best_original_model = max(original_results.items(), \n",
    "                         key=lambda x: x[1]['high_risk_recall'])\n",
    "best_name, best_result = best_original_model\n",
    "\n",
    "print(f\"Optimizing threshold for: {best_name}\")\n",
    "print(f\"Original performance: Recall={best_result['high_risk_recall']:.3f}, \"\n",
    "      f\"Precision={best_result['high_risk_precision']:.3f}\")\n",
    "\n",
    "# Test different recall targets\n",
    "recall_targets = [0.70, 0.75, 0.80, 0.85]\n",
    "threshold_results = {}\n",
    "\n",
    "for target_recall in recall_targets:\n",
    "    try:\n",
    "        opt_threshold, opt_precision, achieved_recall = optimize_threshold_for_recall(\n",
    "            y_te, best_result['y_pred_proba'], target_recall\n",
    "        )\n",
    "        \n",
    "        # Apply this threshold\n",
    "        y_pred_optimized = (best_result['y_pred_proba'] >= opt_threshold).astype(int)\n",
    "        \n",
    "        # Get detailed metrics\n",
    "        opt_report = classification_report(\n",
    "            y_te, y_pred_optimized, \n",
    "            target_names=['Low Risk', 'High Risk'], \n",
    "            output_dict=True, zero_division=0\n",
    "        )\n",
    "        \n",
    "        threshold_results[target_recall] = {\n",
    "            'threshold': opt_threshold,\n",
    "            'precision': opt_precision,\n",
    "            'recall': achieved_recall,\n",
    "            'f1': opt_report['High Risk']['f1-score'],\n",
    "            'accuracy': opt_report['accuracy']\n",
    "        }\n",
    "        \n",
    "        print(f\"Target Recall {target_recall:.2f}: Threshold={opt_threshold:.3f}, \"\n",
    "              f\"Achieved Recall={achieved_recall:.3f}, Precision={opt_precision:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed for target recall {target_recall}: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 5: COMPARISON AND RECOMMENDATIONS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n5. PERFORMANCE COMPARISON AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all results\n",
    "all_improvement_results = {}\n",
    "\n",
    "# Original optimized models\n",
    "for name, result in original_results.items():\n",
    "    all_improvement_results[f\"Optimized_{name}\"] = result\n",
    "\n",
    "# Balanced dataset results\n",
    "for strategy, strategy_results in balanced_results.items():\n",
    "    for name, result in strategy_results.items():\n",
    "        all_improvement_results[f\"{strategy}_{name}\"] = result\n",
    "\n",
    "# Sort by high-risk F1 score\n",
    "sorted_improvements = sorted(\n",
    "    all_improvement_results.items(), \n",
    "    key=lambda x: x[1]['high_risk_f1'], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"HIGH-RISK CLASS PERFORMANCE RANKING:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<4} {'Model':<25} {'Precision':<10} {'Recall':<8} {'F1':<8} {'AUC':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (name, result) in enumerate(sorted_improvements[:10]):\n",
    "    precision = result['high_risk_precision']\n",
    "    recall = result['high_risk_recall']  \n",
    "    f1 = result['high_risk_f1']\n",
    "    auc = result.get('auc', 0)\n",
    "    \n",
    "    print(f\"{i+1:<4} {name[:24]:<25} {precision:<10.3f} {recall:<8.3f} {f1:<8.3f} {auc:<8.3f}\")\n",
    "\n",
    "# Best model analysis\n",
    "if sorted_improvements:\n",
    "    best_name, best_result = sorted_improvements[0]\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST HIGH-RISK PERFORMANCE: {best_name}\")\n",
    "    print(f\"   High-Risk Precision: {best_result['high_risk_precision']:.3f}\")\n",
    "    print(f\"   High-Risk Recall:    {best_result['high_risk_recall']:.3f}\")\n",
    "    print(f\"   High-Risk F1:        {best_result['high_risk_f1']:.3f}\")\n",
    "    \n",
    "    if best_result['high_risk_recall'] >= 0.75:\n",
    "        print(f\"   ‚úÖ EXCELLENT: Catches 75%+ of high-risk patients\")\n",
    "    elif best_result['high_risk_recall'] >= 0.65:\n",
    "        print(f\"   ‚úÖ GOOD: Catches 65%+ of high-risk patients\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è MODERATE: Still missing many high-risk patients\")\n",
    "    \n",
    "    if best_result['high_risk_precision'] >= 0.70:\n",
    "        print(f\"   ‚úÖ EXCELLENT: Low false alarm rate\")\n",
    "    elif best_result['high_risk_precision'] >= 0.60:\n",
    "        print(f\"   ‚úÖ GOOD: Reasonable false alarm rate\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è MODERATE: Higher false alarm rate\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE BEST MODEL FOR FUTURE USE\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\nüìÅ SAVING BEST MODEL FOR FUTURE USE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create model package to save\n",
    "model_package = {\n",
    "    'model_name': best_name,\n",
    "    'model_object': None,\n",
    "    'scaler': None,\n",
    "    'feature_names': feature_cols,\n",
    "    'performance_metrics': {\n",
    "        'high_risk_precision': best_result['high_risk_precision'],\n",
    "        'high_risk_recall': best_result['high_risk_recall'],\n",
    "        'high_risk_f1': best_result['high_risk_f1'],\n",
    "        'auc': best_result.get('auc', 0),\n",
    "        'accuracy': best_result.get('classification_report', {}).get('accuracy', 0)\n",
    "    },\n",
    "    'model_type': None,\n",
    "    'requires_scaling': False,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'training_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'class_distribution': {\n",
    "        'low_risk_count': low_risk_count,\n",
    "        'high_risk_count': high_risk_count,\n",
    "        'imbalance_ratio': imbalance_ratio\n",
    "    }\n",
    "}\n",
    "\n",
    "# Determine which specific model to save based on best_name\n",
    "if best_name == \"Ensemble\":\n",
    "    print(\"‚ö†Ô∏è Note: Ensemble model saving requires special handling\")\n",
    "    print(\"   Saving ensemble metadata and component model references\")\n",
    "    model_package['model_type'] = 'ensemble'\n",
    "    model_package['ensemble_info'] = {\n",
    "        'threshold': ensemble_result['threshold'],\n",
    "        'component_models': list(original_results.keys())\n",
    "    }\n",
    "    # Save ensemble metadata only\n",
    "    # model_filename = f\"best_high_risk_model_ensemble_rs{RANDOM_STATE}.pkl\"\n",
    "    model_filename = f\"patient_predictor_model.pkl\"\n",
    "    \n",
    "else:\n",
    "    # Find and save the actual trained model\n",
    "    model_found = False\n",
    "    \n",
    "    # Check original results first\n",
    "    for orig_name, orig_result in original_results.items():\n",
    "        if best_name == f\"Optimized_{orig_name}\":\n",
    "            model_package['model_object'] = orig_result['model']\n",
    "            model_package['model_type'] = orig_name\n",
    "            \n",
    "            # Check if this model required scaling\n",
    "            if orig_name == \"Weighted_LR\" or orig_name == \"Weighted_SVM\":\n",
    "                model_package['requires_scaling'] = True\n",
    "                # Save the scaler used\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X_tr)  # Refit scaler on training data\n",
    "                model_package['scaler'] = scaler\n",
    "                print(f\"   ‚úÖ Scaling required - StandardScaler saved\")\n",
    "            \n",
    "            model_found = True\n",
    "            break\n",
    "    \n",
    "    # Check balanced dataset results if not found in original\n",
    "    if not model_found:\n",
    "        for strategy, strategy_results in balanced_results.items():\n",
    "            for strat_name, strat_result in strategy_results.items():\n",
    "                if best_name == f\"{strategy}_{strat_name}\":\n",
    "                    model_package['model_object'] = strat_result['model']\n",
    "                    model_package['model_type'] = f\"{strategy}_{strat_name}\"\n",
    "                    \n",
    "                    # Check if this model required scaling\n",
    "                    if strat_name == \"Weighted_LR\" or strat_name == \"Weighted_SVM\":\n",
    "                        model_package['requires_scaling'] = True\n",
    "                        # Get the balanced dataset for this strategy\n",
    "                        X_bal_strat, y_bal_strat = balanced_datasets[strategy]\n",
    "                        X_train_bal, _, _, _ = train_test_split(\n",
    "                            X_bal_strat, y_bal_strat, test_size=0.3, \n",
    "                            random_state=RANDOM_STATE, stratify=y_bal_strat\n",
    "                        )\n",
    "                        scaler = StandardScaler()\n",
    "                        scaler.fit(X_train_bal)  # Fit on balanced training data\n",
    "                        model_package['scaler'] = scaler\n",
    "                        print(f\"   ‚úÖ Scaling required - StandardScaler saved (trained on {strategy} data)\")\n",
    "                    \n",
    "                    model_found = True\n",
    "                    break\n",
    "            if model_found:\n",
    "                break\n",
    "    \n",
    "    model_filename = f\"patient_predictor_model.pkl\"\n",
    "\n",
    "# Save the model package\n",
    "try:\n",
    "    joblib.dump(model_package, model_filename)\n",
    "    print(f\"‚úÖ Model saved successfully: {model_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "\n",
    "print(f\"\\nüìã MODEL PACKAGE CONTENTS:\")\n",
    "print(f\"   ‚Ä¢ Model Type: {model_package['model_type']}\")\n",
    "print(f\"   ‚Ä¢ Requires Scaling: {model_package['requires_scaling']}\")\n",
    "print(f\"   ‚Ä¢ Features Used: {len(model_package['feature_names'])} features\")\n",
    "print(f\"   ‚Ä¢ Performance: F1={model_package['performance_metrics']['high_risk_f1']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Random State: {model_package['random_state']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"HIGH-RISK CLASS IMPROVEMENT ANALYSIS COMPLETE!\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671783f-23b7-4c0b-a053-4ecf3cbc852f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
